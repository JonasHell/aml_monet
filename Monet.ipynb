{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Monet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMQXYRm8GWJ1UZwsNmcok/9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JonasHell/aml_monet/blob/main/Monet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5vQhYuYUPcF_",
        "outputId": "661f1508-34c0-41bc-9b63-f86c499afb73"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6eINb0xeP37g",
        "outputId": "dc3ba692-a629-4ea2-8e97-87e9371d692a"
      },
      "source": [
        "%cd /content/gdrive/MyDrive/CINN"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/CINN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T3wiWVlrP8zn",
        "outputId": "b42ccd57-f4cf-4155-cf1f-05d15830a31e"
      },
      "source": [
        "!git clone https://github.com/JonasHell/aml_monet.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'aml_monet'...\n",
            "fatal: could not read Username for 'https://github.com': No such device or address\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "if_Gw3RMSyd7",
        "outputId": "406c7cee-1e86-4c36-bfca-13480377d524"
      },
      "source": [
        "pip install git+https://github.com/VLL-HD/FrEIA.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/VLL-HD/FrEIA.git\n",
            "  Cloning https://github.com/VLL-HD/FrEIA.git to /tmp/pip-req-build-uvlcukas\n",
            "  Running command git clone -q https://github.com/VLL-HD/FrEIA.git /tmp/pip-req-build-uvlcukas\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from FrEIA==0.2) (1.19.5)\n",
            "Collecting scipy>=1.5\n",
            "  Downloading scipy-1.7.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (28.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 28.5 MB 84 kB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from FrEIA==0.2) (1.9.0+cu102)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.0.0->FrEIA==0.2) (3.7.4.3)\n",
            "Building wheels for collected packages: FrEIA\n",
            "  Building wheel for FrEIA (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for FrEIA: filename=FrEIA-0.2-py3-none-any.whl size=48967 sha256=4169641c358bb375c7e7e515de971b69b1c0edd61bea85ae470316dff30ee168\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-lorzqzei/wheels/ce/20/ea/82f4b732f0705014bd0b340c3375d25aa89491a98a49ec4b8f\n",
            "Successfully built FrEIA\n",
            "Installing collected packages: scipy, FrEIA\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.4.1\n",
            "    Uninstalling scipy-1.4.1:\n",
            "      Successfully uninstalled scipy-1.4.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed FrEIA-0.2 scipy-1.7.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "scipy"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_02i7wBeRY54"
      },
      "source": [
        "#################\n",
        "# Architecture: #\n",
        "#################\n",
        "img_width  = 112\n",
        "img_height = 112\n",
        "\n",
        "#############################\n",
        "# Training hyperparameters: #\n",
        "#############################\n",
        "N_epochs    = 10\n",
        "lr          = 1e-3\n",
        "batch_size  = 16\n",
        "\n",
        "#Total number of images used for training\n",
        "N_train     = 512\n",
        "\n",
        "#We take the first N_test images from the test dataset for validation and the N_val images after that for validation\n",
        "N_test      = 128\n",
        "N_val       = 64\n",
        "\n",
        "#######################\n",
        "# Dataset parameters: #\n",
        "#######################\n",
        "\n",
        "#Root folder for dataset\n",
        "data_root = '/content/gdrive/MyDrive/Daniel-Daten/'\n",
        "\n",
        "#The code expects the images in the training and test folders to be named real1.jpg, real2.jpg, ..., fake1.jpg, fake2.jpg, ...\n",
        "training_img_folder    =  data_root + 'cyclegan train/fake/'\n",
        "training_cond_folder   =  data_root + 'cyclegan train/real/'\n",
        "test_img_folder        =  data_root + 'cyclegan test/fake/' \n",
        "test_cond_folder       =  data_root + 'cyclegan test/real/'\n",
        "\n",
        "########################\n",
        "# Display and logging: #\n",
        "########################\n",
        "\n",
        "output_root = '/content/gdrive/MyDrive/CINN/'\n",
        "output_image_folder     = output_root + 'images'\n",
        "\n",
        "#######################\n",
        "# Saving checkpoints: #\n",
        "#######################\n",
        "\n",
        "#This is where the training algorithm stores the model\n",
        "model_output = output_root + 'monet_cinn.pt'\n",
        "#This is where the evaluation algorithm reas the model from\n",
        "model_path   = model_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_L8imHbReJU",
        "outputId": "bcf7657d-4c35-4336-cca0-8d490b76ec35"
      },
      "source": [
        "import numpy as np\n",
        "from skimage import io, color\n",
        "import torch\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "import torchvision.transforms as T\n",
        "import albumentations\n",
        "\n",
        "\"\"\"\n",
        "Implement dataset for pytorch dataloader that returns source images as well as condition images\n",
        "and create training and test dataloaders.\n",
        "\n",
        "Remember to:\n",
        "- Set correct paths to images\n",
        "- Adapt image size and data augmentation to your needs\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "Custom dataset for holding photo + artistic rendering of photo pairs of RGB images\n",
        "Supports:\n",
        "-   cropping images to desired size (128 pixels x 128 pixels by default)\n",
        "-   normalising images\n",
        "-   data augmentation\n",
        "-   adding noise to training data\n",
        "\n",
        "Returns:\n",
        "-   Tuples of source images with their respective condition images\n",
        "\"\"\"\n",
        "\n",
        "class PairDataset(Dataset):\n",
        "\n",
        "  #Resize dataset, normalise data and augment if is_valid = 0\n",
        "  def __init__(self, image_paths, condition_paths,  transform = True, noise = False, img_height = img_height, img_width = img_width, mean = (0, 0, 0), std  = (1, 1, 1)):\n",
        "    self.image_paths        = image_paths\n",
        "    self.condition_paths    = condition_paths\n",
        "    #Check whether there is the same number of images and condition\n",
        "    assert(len(image_paths) == len(condition_paths))\n",
        "    self.img_height         = img_height\n",
        "    self.img_width          = img_width\n",
        "    self.transform          = transform\n",
        "    self.noise              = noise\n",
        "\n",
        "\n",
        "    if self.transform == False:\n",
        "      self.aug = albumentations.Compose([\n",
        "                                         albumentations.CenterCrop(img_height, img_width, always_apply = True),\n",
        "                                         albumentations.Normalize(mean, std, always_apply = True)\n",
        "      ])\n",
        "    else:\n",
        "      #Apply affine transformations to scale, shift and rotate input images\n",
        "      self.aug = albumentations.Compose([\n",
        "                                         albumentations.RandomCrop(img_height, img_width, always_apply = True),\n",
        "                                         albumentations.Normalize(mean, std, always_apply = True),\n",
        "                                         albumentations.ShiftScaleRotate(shift_limit = 0.0625, \n",
        "                                                                         scale_limit = 0.1, \n",
        "                                                                         rotate_limit = 5,\n",
        "                                                                         p = 0.9)\n",
        "      ])\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.image_paths)\n",
        "\n",
        "  def get_image(self, filename):\n",
        "    #Open image and convert to numpy array \n",
        "    img = np.array(Image.open(filename))\n",
        "    \n",
        "    #Resize to desired size and interpolate if necessary\n",
        "    #Normalise and apply data augmentation\n",
        "    #img = cv2.resize(img, dsize=(self.img_width, self.img_height), interpolation = cv2.INTER_CUBIC)\n",
        "    img = self.aug(image = img)[\"image\"]\n",
        "\n",
        "    #Convert back to pytorch data structure\n",
        "    img = np.transpose(img, (2,0,1)).astype(np.float32) # changing format s.t. pytorch will accept it\n",
        "    img = torch.tensor(img, dtype = torch.float)\n",
        "\n",
        "    #Add noise to improve training performance\n",
        "    if self.noise:\n",
        "        img += 0.005 * torch.rand_like(img)\n",
        "\n",
        "    return img\n",
        "\n",
        "  #Return tuple of image and its condition (another image)\n",
        "  def __getitem__(self, index):\n",
        "    image = self.get_image(self.image_paths[index])\n",
        "    condition = self.get_image(self.condition_paths[index])\n",
        "    return image, condition\n",
        "\n",
        "\n",
        "\n",
        "training_img_list   =  [training_img_folder  + f'fake{i}.jpg' for i in range(1, 1 + N_train)]\n",
        "training_cond_list  =  [training_cond_folder + f'real{i}.jpg' for i in range(1, 1 + N_train)]\n",
        "test_img_list       =  [test_img_folder      + f'fake{i}.jpg' for i in range(1, 1 + N_test)]\n",
        "test_cond_list      =  [test_cond_folder     + f'real{i}.jpg' for i in range(1, 1 + N_test)]\n",
        "val_img_list        =  [test_img_folder      + f'fake{i}.jpg' for i in range(2 + N_test, 2 + N_test + N_val)]\n",
        "val_cond_list       =  [test_cond_folder     + f'real{i}.jpg' for i in range(2 + N_test, 2 + N_test + N_val)]\n",
        "\n",
        "train_data = PairDataset(training_img_list, training_cond_list, transform=True, noise=True)\n",
        "test_data  = PairDataset(test_img_list, test_cond_list        , transform=False, noise=False)\n",
        "val_data  =  PairDataset(val_img_list, val_cond_list          , transform=False, noise=False)\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_data,   batch_size=batch_size, shuffle=True,    num_workers=8,  pin_memory=True, drop_last=True)\n",
        "test_loader  = DataLoader(test_data,    batch_size=batch_size, shuffle=False,   num_workers=4,  pin_memory=True, drop_last=False)\n",
        "\n",
        "#Load all test and validation images and append them to a list\n",
        "#stack concatenates a sequence of tensors along a new dimension\n",
        "#list creates a list using the __get_item__ function\n",
        "x  = list(test_data)\n",
        "tx = list(zip(*x))\n",
        "test_img_all  = torch.stack(tx[0], 0).cuda()\n",
        "test_cond_all  = torch.stack(tx[1], 0).cuda()\n",
        "\n",
        "x  = list(val_data)\n",
        "tx = list(zip(*x))\n",
        "val_img_all  = torch.stack(tx[0], 0).cuda()\n",
        "val_cond_all  = torch.stack(tx[1], 0).cuda()\n",
        "\"\"\"\n",
        "Mit den folgenden Zeilen kann man den Code testen und sich den Effekt der Data Augmentation für das Trainingsdatenset anschauen\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "id = 64\n",
        "img1, img2 = test_data[id]\n",
        "plt.imshow(np.transpose(img1.numpy(), (1, 2, 0)))\n",
        "plt.imshow(np.transpose(img2.numpy(), (1, 2, 0)))\n",
        "\n",
        "\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nMit den folgenden Zeilen kann man den Code testen und sich den Effekt der Data Augmentation für das Trainingsdatenset anschauen\\n\\nimport matplotlib.pyplot as plt\\n%matplotlib inline\\nid = 64\\nimg1, img2 = test_data[id]\\nplt.imshow(np.transpose(img1.numpy(), (1, 2, 0)))\\nplt.imshow(np.transpose(img2.numpy(), (1, 2, 0)))\\n\\n\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdPSlQ0IRiMZ"
      },
      "source": [
        "# %%\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.modules.activation import ReLU\n",
        "import torch.optim\n",
        "\n",
        "import FrEIA.framework as Ff\n",
        "import FrEIA.modules as Fm\n",
        "\n",
        "# %%\n",
        "ndim_total = 3*img_width*img_height\n",
        "\n",
        "class ConditionNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.res_blocks = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                # 3 x 224 x 224\n",
        "                nn.Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
        "                # 64 x 224 x 224\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),\n",
        "                # 64 x 112 x 112\n",
        "                nn.Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
        "                # 128 x 112 x 112\n",
        "            ),\n",
        "            nn.Sequential(\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),\n",
        "                # 128 x 56 x 56\n",
        "                nn.Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
        "                # 256 x 56 x 56\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
        "                # 256 x 56 x 56\n",
        "            ),\n",
        "            nn.Sequential(\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),\n",
        "                # 256 x 28 x 28\n",
        "                nn.Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
        "                # 512 x 28 x 28\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
        "                # 512 x 28 x 28\n",
        "            ),\n",
        "            nn.Sequential(\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),\n",
        "                # 512 x 14 x 14\n",
        "                nn.Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
        "                # 512 x 14 x 14\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
        "                # 512 x 14 x 14\n",
        "            ),\n",
        "            nn.Sequential(\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
        "                # 512 x 7 x 7\n",
        "            ),\n",
        "            nn.Sequential(\n",
        "                nn.AdaptiveAvgPool2d(output_size=(7, 7)),\n",
        "                # 25088\n",
        "                nn.Linear(in_features=25088, out_features=4096, bias=True),\n",
        "                # 4096\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Dropout(p=0.5, inplace=False),\n",
        "                nn.Linear(in_features=4096, out_features=4096, bias=True),\n",
        "                # 4096\n",
        "                #nn.ReLU(inplace=True),\n",
        "                #nn.Dropout(p=0.5, inplace=False), # TODO: with dropout?\n",
        "                #nn.Linear(in_features=4096, out_features=1000, bias=True)\n",
        "                # 1000\n",
        "            )\n",
        "        ])\n",
        "        \n",
        "    def forward(self, photo):\n",
        "        outputs = [photo]\n",
        "        for module in self.res_blocks:\n",
        "            outputs.append(module(outputs[-1]))\n",
        "        return outputs[1:]\n",
        "\n",
        "    # evtl auch möglich:\n",
        "    # https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html\n",
        "    # dann einfach in forward die entsprechenden unter module aufrufen, vermutlich baer ähnlich großer aufwand\n",
        "        \n",
        "\n",
        "class MonetCINN_112_blocks10(nn.Module):\n",
        "    def __init__(self, learning_rate):\n",
        "        super().__init__()\n",
        "\n",
        "        self.cinn = self.create_cinn()\n",
        "        self.cond_net = ConditionNet()\n",
        "\n",
        "        self.trainable_parameters = [p for p in self.cinn.parameters() if p.requires_grad]\n",
        "        for p in self.trainable_parameters:\n",
        "            p.data = 0.02 * torch.randn_like(p)\n",
        "\n",
        "        self.trainable_parameters += list(self.cond_net.parameters())\n",
        "        self.optimizer = torch.optim.Adam(self.trainable_parameters, lr=learning_rate)\n",
        "\n",
        "    def create_cinn(self):\n",
        "    \n",
        "        def subnet_conv(hidden_channels_1, hidden_channels_2, kernel_size):\n",
        "            padding = kernel_size // 2\n",
        "            return lambda in_channels, out_channels: nn.Sequential(\n",
        "                nn.Conv2d(in_channels, hidden_channels_1, kernel_size, padding=padding),\n",
        "                nn.ReLU(),\n",
        "                nn.Conv2d(hidden_channels_1, hidden_channels_2, kernel_size, padding=padding),\n",
        "                nn.ReLU(),\n",
        "                nn.BatchNorm2d(hidden_channels_2),\n",
        "                nn.Conv2d(hidden_channels_2, out_channels, kernel_size, padding=padding)\n",
        "            )\n",
        "\n",
        "        def subnet_fc(hidden_channels_1, hidden_channels_2):\n",
        "            return lambda in_channels, out_channels: nn.Sequential(\n",
        "                nn.Linear(in_channels, hidden_channels_1),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(hidden_channels_1, hidden_channels_2),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(hidden_channels_2, out_channels)\n",
        "            )\n",
        "\n",
        "        def add_stage(nodes, block_num, subnet_func, condition=None, split_nodes=None, split_sizes=None, downsample=True, prefix=''):\n",
        "            \"\"\"\n",
        "            Convenience function that adds an entire stage to nodes.\n",
        "            \"\"\"\n",
        "            #TODO: does appending work correctly?\n",
        "\n",
        "            # add specified number of blocks\n",
        "            for k in range(block_num):\n",
        "                subnet = subnet_func(block_num)\n",
        "                \n",
        "                # add current block\n",
        "                nodes.append(Ff.Node(\n",
        "                    nodes[-1],\n",
        "                    Fm.GLOWCouplingBlock,\n",
        "                    {'subnet_constructor': subnet, 'clamp': 2.0},\n",
        "                    conditions=condition, #TODO\n",
        "                    name=prefix+f'-block{k+1}'\n",
        "                ))\n",
        "\n",
        "                # add permutation after each block\n",
        "                nodes.append(Ff.Node(\n",
        "                    nodes[-1],\n",
        "                    Fm.PermuteRandom,\n",
        "                    {},\n",
        "                    name=prefix+f'-block{k+1}-perm'\n",
        "                ))\n",
        "            #print(nodes[-1])\n",
        "            # split channels off\n",
        "            if split_nodes is not None:\n",
        "                nodes.append(Ff.Node(\n",
        "                    nodes[-1],\n",
        "                    Fm.Split,\n",
        "                    {'section_sizes': split_sizes, 'dim': 0},\n",
        "                    name=prefix+'split'\n",
        "                ))\n",
        "                split_nodes.append(Ff.Node(\n",
        "                    nodes[-1].out1,\n",
        "                    Fm.Flatten,\n",
        "                    {},\n",
        "                    name=prefix+'flatten'\n",
        "                ))\n",
        "\n",
        "            # add downsampling at the end of stage\n",
        "            if downsample:\n",
        "                nodes.append(Ff.Node(\n",
        "                    nodes[-1],\n",
        "                    Fm.HaarDownsampling,\n",
        "                    {'rebalance': 0.5},\n",
        "                    name=prefix+'-down'\n",
        "                ))\n",
        "            \n",
        "        # create nodes with input node\n",
        "        #nodes = [Ff.InputNode(3, 256, 256)]\n",
        "        nodes = [Ff.InputNode(3, 112, 112)]\n",
        "\n",
        "        # create conditions\n",
        "        condition_nodes = [ Ff.ConditionNode(128, 112, 112),\n",
        "                            Ff.ConditionNode(256, 56, 56),\n",
        "                            Ff.ConditionNode(512, 28, 28),\n",
        "                            Ff.ConditionNode(512, 14, 14),\n",
        "                            Ff.ConditionNode(512, 7, 7),\n",
        "                            Ff.ConditionNode(4096)] #TODO: 1000 or 4096?\n",
        "\n",
        "        # create split_nodes\n",
        "        split_nodes = []\n",
        "        \n",
        "        # stage 1\n",
        "        # one block (3 x 112 x 112)\n",
        "        # with conv3 subnet\n",
        "        subnet_func = lambda _: subnet_conv(32, 64, 3)\n",
        "        add_stage(nodes, 1, subnet_func,\n",
        "            condition=condition_nodes[0],\n",
        "            prefix='stage1'\n",
        "        )\n",
        "\n",
        "        # stage 2\n",
        "        # two blocks (12 x 56 x 56)\n",
        "        # one with conv1 and one with conv3 subnet\n",
        "        subnet_func = lambda block_num: subnet_conv(64, 128, 3 if block_num%2 else 1)\n",
        "        add_stage(nodes, 2, subnet_func,\n",
        "            condition=condition_nodes[1],\n",
        "            split_nodes=split_nodes,\n",
        "            prefix='stage2'\n",
        "        )\n",
        "\n",
        "        # stage 3\n",
        "        # two blocks (24 x 28 x 28)\n",
        "        # one with conv1 and one with conv3 subnet\n",
        "        subnet_func = lambda block_num: subnet_conv(128, 256, 3 if block_num%2 else 1)\n",
        "        add_stage(nodes, 2, subnet_func,\n",
        "            condition=condition_nodes[2],\n",
        "            split_nodes=split_nodes,\n",
        "            prefix='stage3'\n",
        "        )\n",
        "\n",
        "        # stage 4\n",
        "        # two blocks (48 x 14 x 14)\n",
        "        # one with conv1 and one with conv3 subnet\n",
        "        subnet_func = lambda block_num: subnet_conv(128, 256, 3 if block_num%2 else 1)\n",
        "        add_stage(nodes, 2, subnet_func,\n",
        "            condition=condition_nodes[3],\n",
        "            split_nodes=split_nodes,\n",
        "            prefix='stage4'\n",
        "        )\n",
        "        #TODO: does it make sense to increase num of channels in subnets?\n",
        "        #TODO: should they be larger in the beginning due to condition\n",
        "        #print(nodes[-1])\n",
        "        # stage 5\n",
        "        # two blocks (96 x 7 x 7)\n",
        "        # one with conv1 and one with conv3 subnet\n",
        "        subnet_func = lambda block_num: subnet_conv(128, 256, 3 if block_num%2 else 1)\n",
        "        add_stage(nodes, 2, subnet_func,\n",
        "            condition=condition_nodes[4],\n",
        "            downsample=False,\n",
        "            split_nodes=split_nodes,\n",
        "            split_sizes=[24, 72],\n",
        "            prefix='stage5'\n",
        "        )\n",
        "\n",
        "        # flatten for fc part\n",
        "        nodes.append(Ff.Node(\n",
        "            nodes[-1],\n",
        "            Fm.Flatten,\n",
        "            {},\n",
        "            name='flatten'\n",
        "        ))\n",
        "\n",
        "        # stage 6\n",
        "        # one block (flat 1176)\n",
        "        # with fc subnetwork\n",
        "        subnet_func = lambda _: subnet_fc(1024, 1024)\n",
        "        add_stage(nodes, 1, subnet_func,\n",
        "            condition=condition_nodes[5],\n",
        "            downsample=False,\n",
        "            prefix='stage6'\n",
        "        )\n",
        "        #print(nodes[-1])\n",
        "        # concat all the splits and the output of fc part\n",
        "        nodes.append(Ff.Node(\n",
        "            [sn.out0 for sn in split_nodes] + [nodes[-1].out0],\n",
        "            Fm.Concat,\n",
        "            {'dim':0},\n",
        "            name='concat'\n",
        "        ))\n",
        "        #print(nodes[-1])\n",
        "        # add output node\n",
        "        nodes.append(Ff.OutputNode(nodes[-1], name='output'))\n",
        "        #print(nodes[-1])\n",
        "        #TODO: use GraphINN or ReversibleGraphNet??\n",
        "        return Ff.ReversibleGraphNet(nodes + split_nodes + condition_nodes)\n",
        "\n",
        "    def forward(self, monet, photo):\n",
        "        z = self.cinn(monet, c=self.cond_net(photo))\n",
        "        jac = self.cinn.log_jacobian(run_forward=False)\n",
        "        return z, jac\n",
        "\n",
        "    def reverse_sample(self, z, photo):\n",
        "        return self.cinn(z, c=self.cond_net(photo), rev=True)\n",
        "\n",
        "        # am ende fc? oder muss conv dann iwie so passen dass am ende was sinvolles rauskommt\n",
        "        # muss latent space gleiche dim wie original space haben\n",
        "        # welche dim müssen conditions haben\n",
        "        # wieviel channel in subnet?"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fG3GoxxWR6X6",
        "outputId": "29f43dda-acba-4eba-86e9-3e29979faf97"
      },
      "source": [
        "from time import time\n",
        "\n",
        "#from tqdm import tqdm\n",
        "import torch\n",
        "import torch.optim\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Training routine for INN\n",
        "Remember to:\n",
        "- Adapt number of epochs\n",
        "- Set learning rate\n",
        "- Set output directory for model\n",
        "\"\"\"\n",
        "\n",
        "cinn = MonetCINN_112_blocks10(lr)\n",
        "cinn.cuda()\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(cinn.optimizer, 1, gamma=0.1)\n",
        "\n",
        "N_epochs = N_epochs\n",
        "t_start = time()\n",
        "nll_mean = []\n",
        "\n",
        "print('Epoch\\tBatch/Total \\tTime \\tNLL train\\tNLL val\\tLR')\n",
        "for epoch in range(N_epochs):\n",
        "    for i, images in enumerate(train_loader):\n",
        "        #train_loader returns a list with two elements\n",
        "        #Both elements are torch tensors with size (batch_size x 3 (RGB channels) x image width x image height)\n",
        "        #We immediately load every batch of source and condition images to the GPU\n",
        "        source    = images[0].cuda()\n",
        "        condition = images[1].cuda()\n",
        "\n",
        "        #We pass both the source image as well as the condition image to the INN\n",
        "        \"\"\"\n",
        "        This needs to be adapted depending on the final architecture of the INN\n",
        "        \"\"\"\n",
        "        z, log_j = cinn(source, condition)\n",
        "\n",
        "        #Compute the loss of the INN\n",
        "        nll = torch.mean(z**2) / 2 - torch.mean(log_j) / ndim_total\n",
        "        nll.backward()\n",
        "        nll_mean.append(nll.item())\n",
        "        cinn.optimizer.step()\n",
        "        cinn.optimizer.zero_grad()\n",
        "\n",
        "        #Print training progress every 20 batches\n",
        "        if not i % 20:\n",
        "            #Compute training loss for validation images\n",
        "            with torch.no_grad():\n",
        "                \"\"\"\n",
        "                This needs to be adapted depending on the final architecture of the INN\n",
        "                \"\"\"\n",
        "                z, log_j = cinn(val_img_all, val_cond_all)\n",
        "                nll_val = torch.mean(z**2) / 2 - torch.mean(log_j) / ndim_total\n",
        "\n",
        "            print('%.3i \\t%.5i/%.5i \\t%.2f \\t%.6f\\t%.6f\\t%.2e' % (epoch,\n",
        "                                                            i, len(train_loader),\n",
        "                                                            (time() - t_start)/60.,\n",
        "                                                            np.mean(nll_mean),\n",
        "                                                            nll_val.item(),\n",
        "                                                            cinn.optimizer.param_groups[0]['lr'],\n",
        "                                                            ), flush=True)\n",
        "            nll_mean = []\n",
        "\n",
        "    scheduler.step()\n",
        "torch.save(cinn.state_dict(), model_output)\n",
        "\n",
        "# %%\n"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ReversibleGraphNet(\n",
            "  (module_list): ModuleList(\n",
            "    (0): GLOWCouplingBlock(\n",
            "      (subnet1): Sequential(\n",
            "        (0): Conv2d(129, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (1): ReLU()\n",
            "        (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (3): ReLU()\n",
            "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (5): Conv2d(64, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      )\n",
            "      (subnet2): Sequential(\n",
            "        (0): Conv2d(130, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (1): ReLU()\n",
            "        (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (3): ReLU()\n",
            "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (5): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      )\n",
            "    )\n",
            "    (1): PermuteRandom()\n",
            "    (2): HaarDownsampling()\n",
            "    (3): GLOWCouplingBlock(\n",
            "      (subnet1): Sequential(\n",
            "        (0): Conv2d(262, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (1): ReLU()\n",
            "        (2): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (3): ReLU()\n",
            "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (5): Conv2d(128, 12, kernel_size=(1, 1), stride=(1, 1))\n",
            "      )\n",
            "      (subnet2): Sequential(\n",
            "        (0): Conv2d(262, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (1): ReLU()\n",
            "        (2): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (3): ReLU()\n",
            "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (5): Conv2d(128, 12, kernel_size=(1, 1), stride=(1, 1))\n",
            "      )\n",
            "    )\n",
            "    (4): PermuteRandom()\n",
            "    (5): GLOWCouplingBlock(\n",
            "      (subnet1): Sequential(\n",
            "        (0): Conv2d(262, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (1): ReLU()\n",
            "        (2): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (3): ReLU()\n",
            "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (5): Conv2d(128, 12, kernel_size=(1, 1), stride=(1, 1))\n",
            "      )\n",
            "      (subnet2): Sequential(\n",
            "        (0): Conv2d(262, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (1): ReLU()\n",
            "        (2): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (3): ReLU()\n",
            "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (5): Conv2d(128, 12, kernel_size=(1, 1), stride=(1, 1))\n",
            "      )\n",
            "    )\n",
            "    (6): PermuteRandom()\n",
            "    (7): Split()\n",
            "    (8): HaarDownsampling()\n",
            "    (9): GLOWCouplingBlock(\n",
            "      (subnet1): Sequential(\n",
            "        (0): Conv2d(524, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (1): ReLU()\n",
            "        (2): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (3): ReLU()\n",
            "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (5): Conv2d(256, 24, kernel_size=(1, 1), stride=(1, 1))\n",
            "      )\n",
            "      (subnet2): Sequential(\n",
            "        (0): Conv2d(524, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (1): ReLU()\n",
            "        (2): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (3): ReLU()\n",
            "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (5): Conv2d(256, 24, kernel_size=(1, 1), stride=(1, 1))\n",
            "      )\n",
            "    )\n",
            "    (10): PermuteRandom()\n",
            "    (11): GLOWCouplingBlock(\n",
            "      (subnet1): Sequential(\n",
            "        (0): Conv2d(524, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (1): ReLU()\n",
            "        (2): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (3): ReLU()\n",
            "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (5): Conv2d(256, 24, kernel_size=(1, 1), stride=(1, 1))\n",
            "      )\n",
            "      (subnet2): Sequential(\n",
            "        (0): Conv2d(524, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (1): ReLU()\n",
            "        (2): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (3): ReLU()\n",
            "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (5): Conv2d(256, 24, kernel_size=(1, 1), stride=(1, 1))\n",
            "      )\n",
            "    )\n",
            "    (12): PermuteRandom()\n",
            "    (13): Split()\n",
            "    (14): HaarDownsampling()\n",
            "    (15): GLOWCouplingBlock(\n",
            "      (subnet1): Sequential(\n",
            "        (0): Conv2d(536, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (1): ReLU()\n",
            "        (2): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (3): ReLU()\n",
            "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (5): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1))\n",
            "      )\n",
            "      (subnet2): Sequential(\n",
            "        (0): Conv2d(536, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (1): ReLU()\n",
            "        (2): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (3): ReLU()\n",
            "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (5): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1))\n",
            "      )\n",
            "    )\n",
            "    (16): PermuteRandom()\n",
            "    (17): GLOWCouplingBlock(\n",
            "      (subnet1): Sequential(\n",
            "        (0): Conv2d(536, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (1): ReLU()\n",
            "        (2): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (3): ReLU()\n",
            "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (5): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1))\n",
            "      )\n",
            "      (subnet2): Sequential(\n",
            "        (0): Conv2d(536, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (1): ReLU()\n",
            "        (2): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (3): ReLU()\n",
            "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (5): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1))\n",
            "      )\n",
            "    )\n",
            "    (18): PermuteRandom()\n",
            "    (19): Split()\n",
            "    (20): HaarDownsampling()\n",
            "    (21): GLOWCouplingBlock(\n",
            "      (subnet1): Sequential(\n",
            "        (0): Conv2d(560, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (1): ReLU()\n",
            "        (2): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (3): ReLU()\n",
            "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (5): Conv2d(256, 96, kernel_size=(1, 1), stride=(1, 1))\n",
            "      )\n",
            "      (subnet2): Sequential(\n",
            "        (0): Conv2d(560, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (1): ReLU()\n",
            "        (2): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (3): ReLU()\n",
            "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (5): Conv2d(256, 96, kernel_size=(1, 1), stride=(1, 1))\n",
            "      )\n",
            "    )\n",
            "    (22): PermuteRandom()\n",
            "    (23): GLOWCouplingBlock(\n",
            "      (subnet1): Sequential(\n",
            "        (0): Conv2d(560, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (1): ReLU()\n",
            "        (2): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (3): ReLU()\n",
            "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (5): Conv2d(256, 96, kernel_size=(1, 1), stride=(1, 1))\n",
            "      )\n",
            "      (subnet2): Sequential(\n",
            "        (0): Conv2d(560, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (1): ReLU()\n",
            "        (2): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (3): ReLU()\n",
            "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (5): Conv2d(256, 96, kernel_size=(1, 1), stride=(1, 1))\n",
            "      )\n",
            "    )\n",
            "    (24): PermuteRandom()\n",
            "    (25): Split()\n",
            "    (26): Flatten()\n",
            "    (27): GLOWCouplingBlock(\n",
            "      (subnet1): Sequential(\n",
            "        (0): Linear(in_features=4684, out_features=1024, bias=True)\n",
            "        (1): ReLU()\n",
            "        (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        (3): ReLU()\n",
            "        (4): Linear(in_features=1024, out_features=1176, bias=True)\n",
            "      )\n",
            "      (subnet2): Sequential(\n",
            "        (0): Linear(in_features=4684, out_features=1024, bias=True)\n",
            "        (1): ReLU()\n",
            "        (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        (3): ReLU()\n",
            "        (4): Linear(in_features=1024, out_features=1176, bias=True)\n",
            "      )\n",
            "    )\n",
            "    (28): Flatten()\n",
            "    (29): PermuteRandom()\n",
            "    (30): Flatten()\n",
            "    (31): Flatten()\n",
            "    (32): Flatten()\n",
            "    (33): Concat()\n",
            "  )\n",
            ")\n",
            "Epoch\tBatch/Total \tTime \tNLL train\tNLL val\tLR\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-d9e5803a54ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mThis\u001b[0m \u001b[0mneeds\u001b[0m \u001b[0mto\u001b[0m \u001b[0mbe\u001b[0m \u001b[0madapted\u001b[0m \u001b[0mdepending\u001b[0m \u001b[0mon\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfinal\u001b[0m \u001b[0marchitecture\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mINN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \"\"\"\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_j\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcinn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcondition\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;31m#Compute the loss of the INN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-49-0d802e176699>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, monet, photo)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphoto\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m         \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcinn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcond_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphoto\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    279\u001b[0m         \u001b[0mjac\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcinn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_jacobian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_forward\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjac\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-49-0d802e176699>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, photo)\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mphoto\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mres_blocks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m             \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1845\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1847\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: CUBLAS_STATUS_INVALID_VALUE when calling `cublasSgemm( handle, opa, opb, m, n, k, &alpha, a, lda, b, ldb, &beta, c, ldc)`"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWHqHj1FXCut"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}